{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project FaceComp Controller\n",
    "\n",
    "Controlling the computer through facial expressions. Learns the expressions of different people by analyzing them. Trained on more than 30K sample data.\n",
    "\n",
    "<img src = \"https://cdn.pixabay.com/photo/2023/05/31/15/54/ai-generated-8031745_1280.jpg\" width = \"40%\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "from mediapipe.tasks.python import vision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing transfer learning\n",
    "\n",
    "To achieve better and faster results from pre trained models\n",
    "\n",
    "<img src = \"https://cdn.pixabay.com/photo/2017/09/08/19/05/a-2729782_1280.png\" width = \"40%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_options = python.BaseOptions(\n",
    "    model_asset_path = \"./MediapipeExperiments/face_landmarker.task\"\n",
    ")\n",
    "face_mesh = mp.solutions.face_mesh.FaceMesh(\n",
    "    max_num_faces = 1,\n",
    "    static_image_mode = False\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Annotator\n",
    "\n",
    "To marks important landmarks of face, and extract coordinates of target landmarks. All the coordinates are normalized ie: in the scale of [0, 1]. Therefore to get the exact coordinates, mutiply them by the height and width of image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_annotator(detection_result, image, special_pt_list):\n",
    "\n",
    "    \"\"\"\n",
    "    args:\n",
    "    detection_result: The important landmarks detected by pre trained model\n",
    "    image: Live feed from OpenCV cam\n",
    "    special_pt_list: target landmarks than need to be tracked and analyzed to detect facial expressions\n",
    "    \"\"\"\n",
    "    \n",
    "    radius = 1\n",
    "    color = (0, 0, 255)\n",
    "    counter = 1\n",
    "\n",
    "    # For tracking targeted landmarks\n",
    "    special_pt_coordinate = []\n",
    "\n",
    "    # Try except block to tackle the case when model failed to detect landmarks from the face\n",
    "    try:\n",
    "\n",
    "        for landmark in detection_result.multi_face_landmarks[0].landmark:\n",
    "\n",
    "            # If current landmark is a special one\n",
    "            if counter in special_pt_list:\n",
    "\n",
    "                # Cause the coordinates are normalized\n",
    "                x = int(landmark.x*width)\n",
    "                y = int(landmark.y*height)\n",
    "                cv2.circle(image, (x, y), 2, (0,255,0), -1)\n",
    "                counter += 1\n",
    "\n",
    "                special_pt_coordinate.append([landmark.x, landmark.y, landmark.z])\n",
    "\n",
    "                continue\n",
    "\n",
    "            # If it's an ordinary one\n",
    "            x = int(landmark.x*width)\n",
    "            y = int(landmark.y*height)\n",
    "            cv2.circle(image, (x, y), radius, color, -1)\n",
    "\n",
    "            counter += 1\n",
    "\n",
    "        return [image, special_pt_coordinate]\n",
    "    \n",
    "    except Exception as e:\n",
    "        return [image, None]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Live feed input from cam\n",
    "Through OpenCV\n",
    "\n",
    "<img src = \"https://cdn.pixabay.com/photo/2013/07/13/11/26/film-158157_1280.png\" width = \"40%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture = cv2.VideoCapture(0)\n",
    "# Setting the output screen size\n",
    "cv2.namedWindow(\"Video\", cv2.WINDOW_NORMAL)\n",
    "cv2.setWindowProperty(\"Video\", cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining special points\n",
    "\n",
    "<img src = \"https://cdn.pixabay.com/photo/2017/10/29/14/02/portrait-2899779_1280.png\" width = \"40%\">\n",
    "\n",
    "In all, the model outputs 468 coordinates, here's the list of indices those coordinates represent. Coordinates are of the form [x, y, z]\n",
    "\n",
    "- Nose (10 landmarks)\n",
    "\n",
    "- Nose tip: 1\n",
    "- Bridge of the nose: 2-5\n",
    "- Nostrils: 6-9\n",
    "- Left eye (132 landmarks)\n",
    "\n",
    "- Eyebrow (upper lid): 33-42\n",
    "- Eyelid (upper lid): 159-182\n",
    "- Eyebrow (lower lid): 43-52\n",
    "- Eyelid (lower lid): 263-286\n",
    "- Eye (iris): 467\n",
    "- Eye (pupil): 466\n",
    "- Right eye (132 landmarks)\n",
    "\n",
    "- Eyebrow (upper lid): 55-64\n",
    "- Eyelid (upper lid): 374-397\n",
    "- Eyebrow (lower lid): 65-74\n",
    "- Eyelid (lower lid): 475-498\n",
    "- Eye (iris): 468\n",
    "- Eye (pupil): 469\n",
    "- Lips (20 landmarks)\n",
    "\n",
    "- Outer lips (top): 76-95\n",
    "- Outer lips (bottom): 152-171\n",
    "- Inner lips (top): 96-113\n",
    "- Inner lips (bottom): 172-189\n",
    "- Face outline (294 landmarks)\n",
    "\n",
    "- Jawline: 0-16\n",
    "- Cheek: 195-233\n",
    "- Forehead: 234-280"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining special points\n",
    "\n",
    "For tracking and analyzing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index number of those landmarks out of 468 points\n",
    "special_pt_indices = [i for i in range(374, 397)]+[i for i in range(475, 498)] + [i for i in range(159, 182)] + [i for i in range(263, 286)]\n",
    "# Storing their coordinates to track them\n",
    "special_pts_coordinate = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing live feed\n",
    "\n",
    "Detecting the landmarks and programming what to do with them\n",
    "\n",
    "<img src = \"https://cdn.pixabay.com/photo/2017/01/29/22/16/cycle-2019530_1280.png\" width = \"40%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "\n",
    "    success, image = capture.read()\n",
    "    [height, width, channels] = image.shape\n",
    "    results = face_mesh.process(image)\n",
    "    annotated_image = image_annotator(results, image, special_pt_indices)\n",
    "\n",
    "    if annotated_image[1] != None: # If model didn't fail to detect landmarks\n",
    "        # Storing coordinates\n",
    "        print(annotated_image[1])\n",
    "        special_pts_coordinate.append(annotated_image[1])\n",
    "        cv2.imshow(\"Video\", annotated_image[0])\n",
    "\n",
    "    else:\n",
    "        cv2.imshow(\"Video\", annotated_image[0])\n",
    "        \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): # Press q keyboard button to terminate the program\n",
    "        np.save(\"data.npy\", np.array(special_pts_coordinate)) # Storing tracked landmarks in numpy array format\n",
    "        capture.release()\n",
    "        break\n",
    "\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
